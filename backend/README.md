# Neuro-Sentry Backend

A FastAPI-based backend with **automatic Ollama model detection**, **GPU-optimized LLM support**, **comprehensive logging**, and **security threat analysis**.

Neuro-Sentry is designed to intelligently select the best available LLM (CPU or GPU), provide real-time inference, and maintain detailed operational logs for observability and debugging.

---

## ğŸš€ Features

- âœ… **Auto-detects available Ollama models** Priority order:
  1. `llama3-gpu` (GPU-optimized)
  2. `llama3`
  3. `mistral`
  4. First available model
- âš¡ **GPU-accelerated LLM inference** (when available)
- ğŸ›¡ï¸ **Security threat detection & prompt analysis**
- ğŸ“Š **Usage & statistics tracking**
- ğŸ“ **Comprehensive backend logging**

---

## ğŸ“‚ Project Structure

```text
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ backend_TIMESTAMP.log
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ› ï¸ Quick StartFrom the backend directory:Setup (First Time Only)Bashpython3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
Run the BackendBashsource venv/bin/activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
The server will start at: http://localhost:8000ğŸ“œ LoggingAll backend logs are automatically stored in: backend/logs/backend_TIMESTAMP.logView Logs in Real TimeBashtail -f backend/logs/backend_*.log
Logs include:Model detection resultsAPI requests & responsesLLM interaction eventsError traces & system warningsğŸ§  Model Detection LogicNeuro-Sentry automatically detects and selects the best available Ollama model at startup based on the following selection priority:llama3-gpu â€“ GPU-optimized (preferred)llama3 â€“ CPU fallbackmistral â€“ Alternative fallbackAny available modelYou can verify the active model via backend logs or the GET /health endpoint.ğŸ”Œ API EndpointsMethodEndpointDescriptionGET/Service infoGET/healthHealth check & active modelPOST/chatDirect LLM chatPOST/api/promptSecurity threat analysisGET/api/statsUsage statisticsGET/api/logsRecent backend logsğŸï¸ Ollama GPU Model: llama3-gpuOverviewThis backend supports a GPU-forced Ollama model named llama3-gpu, created using a custom Modelfile to ensure consistent GPU offloading.Key Concept: Ollama models are neither CPU-only nor GPU-only. GPU utilization is determined at runtime via configuration parametersâ€”specifically num_gpu.Modelfile ConfigurationFilename: ModelfileDockerfileFROM llama3

PARAMETER num_gpu 99
PARAMETER num_ctx 4096
PARAMETER num_keep 24
Parameter ExplanationParameterDescriptionFROM llama3Uses default llama3 model as basenum_gpu 99Forces maximum GPU layer offloading (auto-clamped by VRAM)num_ctx 4096Sets context window sizenum_keep 24Preserves tokens across turnsCreation & VerificationBash# Create the model
ollama create llama3-gpu -f Modelfile

# Verify configuration
ollama show llama3-gpu
Confirming GPU UtilizationRun the Model: ollama run llama3-gpuMonitor GPU: Run nvidia-smi in a separate terminal. Increased VRAM usage confirms success.ğŸ“ˆ GPU Tuning ReferenceAvailable VRAMRecommended num_gpu4 GB35â€“456 GB55â€“708 GB80â€“9912 GB+99Note: Setting num_gpu to 99 is safeâ€”Ollama automatically clamps the value based on your hardware's actual capacity.ğŸ”® Future Improvements[ ] Automatic GPU VRAM detection[ ] Dynamic num_gpu tuning at runtime[ ] Enhanced performance metrics[ ] Extended security rule sets
